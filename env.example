# LLM Provider Configuration
# Choose your LLM provider: "openai", "ollama", "lmstudio", "vllm"
LLM_PROVIDER=openai

# Set the model name for the selected provider
LLM_MODEL_NAME=gpt-4o

# Set the API key for the selected provider (e.g., OPENAI_API_KEY)
# For local models that don't require an API key, this can be left blank.
LLM_API_KEY=your_openai_api_key_here

# Set the base URL for local LLM providers (Ollama, LMStudio, vLLM)
# Example for Ollama: LLM_API_BASE_URL=http://localhost:11434/v1
# Example for LMStudio: LLM_API_BASE_URL=http://localhost:1234/v1
LLM_API_BASE_URL=

# For backward compatibility, you can still use OPENAI_API_KEY if LLM_API_KEY is not set.
OPENAI_API_KEY=your_openai_api_key_here

# MCP Server URLs (Docker containers)
BRAVE_SEARCH_MCP_URL=http://localhost:3001
GITHUB_MCP_URL=http://localhost:3002
HACKER_NEWS_MCP_URL=http://localhost:3003
FILESYSTEM_MCP_URL=http://localhost:3004

# GitHub Token for MCP Server
GITHUB_TOKEN=your_github_token_here
GITHUB_PERSONAL_ACCESS_TOKEN=your_github_token_here

# Brave Search API Key
BRAVE_API_KEY=your_brave_api_key_here

# Application Configuration
LOG_LEVEL=INFO
ENVIRONMENT=development

# Hacker News Configuration
HN_STORIES_LIMIT=50

# Brave Search Configuration
BRAVE_WEB_SEARCH_LIMIT=20
BRAVE_IMAGE_SEARCH_LIMIT=20
BRAVE_NEWS_SEARCH_LIMIT=20
BRAVE_VIDEO_SEARCH_LIMIT=20
BRAVE_SUMMARIZER_LIMIT=10

# FastAPI Configuration
HOST=0.0.0.0
PORT=8000
WORKERS=1

# A2A Configuration
A2A_SERVER_HOST=localhost
A2A_SERVER_PORT=8001
